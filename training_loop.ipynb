{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MultiEurlexDataset import MultiEurlexDataset\n",
    "from MultiEvalDataset import MultiEvalDataset\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizerFast, TrainingArguments, Trainer, DataCollatorWithPadding, TextClassificationPipeline, EarlyStoppingCallback\n",
    "from torch.utils.data import RandomSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base', padding=False, trunaction=False)\n",
    "#training_dataset = MultiEvalDataset(languages = ['hr'])\n",
    "#lengths = []\n",
    "#for i in range(len(training_dataset)):\n",
    "#    lengths.append(len(training_dataset[i][0].split(' ')))\n",
    "#for i in [10, 20, 30, 40, 50, 60, 70, 80, 90]:\n",
    "#    print(np.percentile(lengths, i, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset multi_eurlex (/home/mrajnovic/.cache/huggingface/datasets/multi_eurlex/all_languages/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecb2b7289674769bfbae1f05d9a9dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% of dataset loaded\n",
      "10.0% of dataset loaded\n",
      "20.0% of dataset loaded\n",
      "30.0% of dataset loaded\n",
      "40.0% of dataset loaded\n",
      "50.0% of dataset loaded\n",
      "60.0% of dataset loaded\n",
      "70.0% of dataset loaded\n",
      "80.0% of dataset loaded\n",
      "90.0% of dataset loaded\n",
      "Loading dataset done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset multi_eurlex (/home/mrajnovic/.cache/huggingface/datasets/multi_eurlex/all_languages/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0e4be04cc548d3a0b7987029324632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% of dataset loaded\n",
      "10.0% of dataset loaded\n",
      "20.0% of dataset loaded\n",
      "30.0% of dataset loaded\n",
      "40.0% of dataset loaded\n",
      "50.0% of dataset loaded\n",
      "60.0% of dataset loaded\n",
      "70.0% of dataset loaded\n",
      "80.0% of dataset loaded\n",
      "90.0% of dataset loaded\n",
      "Loading dataset done\n"
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base', padding=True, trunaction=True)\n",
    "training_dataset = MultiEurlexDataset(languages = ['en'], tokenizer=tokenizer)\n",
    "eval_dataset = MultiEurlexDataset(split='validation',languages = ['hr'], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset multi_eurlex (/home/mrajnovic/.cache/huggingface/datasets/multi_eurlex/all_languages/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4e968a07f84b11be6c7e448f6ec532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% of dataset loaded\n",
      "10.0% of dataset loaded\n",
      "20.0% of dataset loaded\n",
      "30.0% of dataset loaded\n",
      "40.0% of dataset loaded\n",
      "50.0% of dataset loaded\n",
      "60.0% of dataset loaded\n",
      "70.0% of dataset loaded\n",
      "80.0% of dataset loaded\n",
      "90.0% of dataset loaded\n",
      "Loading dataset done\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MultiEurlexDataset(split='test',languages = ['hr'], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator_fn = DataCollatorWithPadding(tokenizer,return_tensors=\"pt\", padding=\"max_length\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    label_pred, label_true = eval_pred\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    label_pred=sigmoid(torch.tensor(label_pred)).numpy()\n",
    "    label_pred[label_pred < 0.5] = 0\n",
    "    label_pred[label_pred >= 0.5] = 1\n",
    "    result={}\n",
    "    result['f1'] = metric(label_true,label_pred, average='samples')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/germanic',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_ratio=0.1,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.001,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=1000,\n",
    "    learning_rate=1e-5,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    lr_scheduler_type='polynomial',\n",
    "    log_level='info',\n",
    "    dataloader_num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=21, problem_type=\"multi_label_classification\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    \n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collator_fn,\n",
    "    #callbacks=[\n",
    "    #    EarlyStoppingCallback(early_stopping_patience=5),\n",
    "    #]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, function_to_apply='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 55000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 68750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='840' max='68750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  840/68750 05:07 < 6:55:36, 2.72 it/s, Epoch 0.12/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-3.169377  , -4.543096  , -3.7680247 , ...,  0.7034655 ,\n",
       "        -2.6702948 , -3.112781  ],\n",
       "       [-2.285403  , -3.7878382 , -4.3809805 , ...,  2.5344818 ,\n",
       "        -3.963364  , -4.3044024 ],\n",
       "       [-2.0230477 , -3.7235363 , -3.9355998 , ..., -1.0295571 ,\n",
       "        -3.3179023 , -2.5032394 ],\n",
       "       ...,\n",
       "       [-4.177247  , -0.31297272, -3.3858912 , ..., -2.6987107 ,\n",
       "        -3.5008006 , -3.5991797 ],\n",
       "       [-1.1728051 , -2.9491198 , -2.7745988 , ...,  0.54032177,\n",
       "        -2.884429  , -2.394803  ],\n",
       "       [-4.5849667 , -5.0209565 , -1.4495361 , ..., -1.1971147 ,\n",
       "        -3.9299302 , -5.317438  ]], dtype=float32), label_ids=array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 1., 0., ..., 1., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32), metrics={'test_loss': 0.2505229711532593, 'test_f1': 0.6447666089466089, 'test_runtime': 62.7384, 'test_samples_per_second': 79.696, 'test_steps_per_second': 4.989})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2594,  2.0396, -3.9613,  1.3616, -2.1823, -3.1136, -3.2287,  0.2411,\n",
       "         -2.1513, -3.2607, -3.1584, -2.7092, -0.9786, -3.5652, -1.6449, -1.1268,\n",
       "         -1.3294, -3.5044, -3.3167, -3.3110,  0.1766]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=training_dataset[0]['input_ids'].unsqueeze(0).to(device),attention_mask=training_dataset[0]['attention_mask'].unsqueeze(0).to(device))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab60847f016ca05497719c81611e5a3da76ef1597f0d5aec9b876a6fbb687fc2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
