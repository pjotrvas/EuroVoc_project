{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MultiEurlexDataset import MultiEurlexDataset\n",
    "from MultiEvalDataset import MultiEvalDataset\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizerFast, TrainingArguments, Trainer, DataCollatorWithPadding, TextClassificationPipeline, EarlyStoppingCallback\n",
    "from torch.utils.data import RandomSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base', padding=False, trunaction=False)\n",
    "#training_dataset = MultiEvalDataset(languages = ['hr'])\n",
    "#lengths = []\n",
    "#for i in range(len(training_dataset)):\n",
    "#    lengths.append(len(training_dataset[i][0].split(' ')))\n",
    "#for i in [10, 20, 30, 40, 50, 60, 70, 80, 90]:\n",
    "#    print(np.percentile(lengths, i, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset multi_eurlex (/home/mrajnovic/.cache/huggingface/datasets/multi_eurlex/all_languages/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0afe2d2172422e8894492fc2bd4b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% of dataset loaded\n",
      "10.0% of dataset loaded\n",
      "20.0% of dataset loaded\n",
      "30.0% of dataset loaded\n",
      "40.0% of dataset loaded\n",
      "50.0% of dataset loaded\n",
      "60.0% of dataset loaded\n",
      "70.0% of dataset loaded\n",
      "80.0% of dataset loaded\n",
      "90.0% of dataset loaded\n",
      "Loading dataset done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset multi_eurlex (/home/mrajnovic/.cache/huggingface/datasets/multi_eurlex/all_languages/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b89edc326c544b3fbfc1b40483214e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% of dataset loaded\n",
      "10.0% of dataset loaded\n",
      "20.0% of dataset loaded\n",
      "30.0% of dataset loaded\n",
      "40.0% of dataset loaded\n",
      "50.0% of dataset loaded\n",
      "60.0% of dataset loaded\n",
      "70.0% of dataset loaded\n",
      "80.0% of dataset loaded\n",
      "90.0% of dataset loaded\n",
      "Loading dataset done\n"
     ]
    }
   ],
   "source": [
    "device='cuda'\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base', padding=True, trunaction=True)\n",
    "training_dataset = MultiEurlexDataset(languages = ['en'], tokenizer=tokenizer)\n",
    "eval_dataset = MultiEurlexDataset(split='validation',languages = ['hr'], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset multi_eurlex (/home/mrajnovic/.cache/huggingface/datasets/multi_eurlex/all_languages/1.0.0/8ec8b79877a517369a143ead6679d1788d13e51cf641ed29772f4449e8364fb6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb6d2bdb28e4231920c10e31690e24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% of dataset loaded\n",
      "10.0% of dataset loaded\n",
      "20.0% of dataset loaded\n",
      "30.0% of dataset loaded\n",
      "40.0% of dataset loaded\n",
      "50.0% of dataset loaded\n",
      "60.0% of dataset loaded\n",
      "70.0% of dataset loaded\n",
      "80.0% of dataset loaded\n",
      "90.0% of dataset loaded\n",
      "Loading dataset done\n"
     ]
    }
   ],
   "source": [
    "test_dataset = MultiEurlexDataset(split='test',languages = ['hr'], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator_fn = DataCollatorWithPadding(tokenizer,return_tensors=\"pt\", padding=\"max_length\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = f1_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    label_pred, label_true = eval_pred\n",
    "    sigmoid = nn.Sigmoid()\n",
    "    label_pred=sigmoid(torch.tensor(label_pred)).numpy()\n",
    "    label_pred[label_pred < 0.5] = 0\n",
    "    label_pred[label_pred >= 0.5] = 1\n",
    "    result={}\n",
    "    result['f1'] = metric(label_true,label_pred, average='samples')\n",
    "    result['classification_report'] = classification_report(label_true,label_pred)\n",
    "    result['confusion_matrix'] = confusion_matrix(label_true, label_pred)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/germanic',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_ratio=0.1,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.001,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=1000,\n",
    "    learning_rate=1e-5,\n",
    "    save_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    lr_scheduler_type='polynomial',\n",
    "    log_level='info',\n",
    "    dataloader_num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/mrajnovic/EuroVoc_project/results/slavic_croatian/checkpoint-72795/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"multi_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file /home/mrajnovic/EuroVoc_project/results/slavic_croatian/checkpoint-72795/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at /home/mrajnovic/EuroVoc_project/results/slavic_croatian/checkpoint-72795.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = XLMRobertaForSequenceClassification.from_pretrained('/home/mrajnovic/EuroVoc_project/results/slavic_croatian/checkpoint-72795', num_labels=21, problem_type=\"multi_label_classification\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    \n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collator_fn,\n",
    "    #callbacks=[\n",
    "    #    EarlyStoppingCallback(early_stopping_patience=5),\n",
    "    #]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, function_to_apply='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 55000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 68750\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='840' max='68750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  840/68750 05:07 < 6:55:36, 2.72 it/s, Epoch 0.12/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 01:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mrajnovic/EuroVoc_project/training_loop.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btuga.takelab.fer.hr/home/mrajnovic/EuroVoc_project/training_loop.ipynb#ch0000016vscode-remote?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mpredict(test_dataset)\n",
      "File \u001b[0;32m~/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py:2358\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2354'>2355</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2356'>2357</a>\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[0;32m-> <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2357'>2358</a>\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2358'>2359</a>\u001b[0m     test_dataloader, description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPrediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2359'>2360</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2360'>2361</a>\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2361'>2362</a>\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2362'>2363</a>\u001b[0m     speed_metrics(\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2363'>2364</a>\u001b[0m         metric_key_prefix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2367'>2368</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2368'>2369</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py:2535\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2532'>2533</a>\u001b[0m \u001b[39m# Metrics!\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2533'>2534</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m all_preds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m all_labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2534'>2535</a>\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2535'>2536</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/transformers/trainer.py?line=2536'>2537</a>\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;32m/home/mrajnovic/EuroVoc_project/training_loop.ipynb Cell 10'\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btuga.takelab.fer.hr/home/mrajnovic/EuroVoc_project/training_loop.ipynb#ch0000009vscode-remote?line=9'>10</a>\u001b[0m result[\u001b[39m'\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m metric(label_true,label_pred, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btuga.takelab.fer.hr/home/mrajnovic/EuroVoc_project/training_loop.ipynb#ch0000009vscode-remote?line=10'>11</a>\u001b[0m result[\u001b[39m'\u001b[39m\u001b[39mclassification_report\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m classification_report(label_true,label_pred)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btuga.takelab.fer.hr/home/mrajnovic/EuroVoc_project/training_loop.ipynb#ch0000009vscode-remote?line=11'>12</a>\u001b[0m result[\u001b[39m'\u001b[39m\u001b[39mconfusion_matrix\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m confusion_matrix(label_true, label_pred)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btuga.takelab.fer.hr/home/mrajnovic/EuroVoc_project/training_loop.ipynb#ch0000009vscode-remote?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/tar/lib/python3.8/site-packages/sklearn/metrics/_classification.py:309\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=306'>307</a>\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=307'>308</a>\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=308'>309</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n\u001b[1;32m    <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=310'>311</a>\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/mrajnovic/miniconda3/envs/tar/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=311'>312</a>\u001b[0m     labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\n",
      "\u001b[0;31mValueError\u001b[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-1.2594,  2.0396, -3.9613,  1.3616, -2.1823, -3.1136, -3.2287,  0.2411,\n",
       "         -2.1513, -3.2607, -3.1584, -2.7092, -0.9786, -3.5652, -1.6449, -1.1268,\n",
       "         -1.3294, -3.5044, -3.3167, -3.3110,  0.1766]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=training_dataset[0]['input_ids'].unsqueeze(0).to(device),attention_mask=training_dataset[0]['attention_mask'].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67       774\n",
      "           1       0.78      0.70      0.74       653\n",
      "           2       0.85      0.85      0.85       974\n",
      "           3       0.92      0.71      0.80      2769\n",
      "           4       0.65      0.75      0.69       394\n",
      "           5       0.82      0.79      0.80      1086\n",
      "           6       0.85      0.88      0.86      1960\n",
      "           7       0.70      0.59      0.65       622\n",
      "           8       0.91      0.81      0.86       600\n",
      "           9       0.34      0.60      0.43       164\n",
      "          10       0.53      0.46      0.49       335\n",
      "          11       0.59      0.58      0.59       531\n",
      "          12       0.67      0.62      0.65       513\n",
      "          13       0.53      0.30      0.39       102\n",
      "          14       0.85      0.67      0.75       185\n",
      "          15       0.80      0.81      0.80      1347\n",
      "          16       0.36      0.21      0.27        57\n",
      "          17       0.96      0.95      0.95      1641\n",
      "          18       0.93      0.91      0.92      2208\n",
      "          19       0.64      0.59      0.62       444\n",
      "          20       0.65      0.59      0.62       610\n",
      "\n",
      "   micro avg       0.81      0.77      0.79     17969\n",
      "   macro avg       0.71      0.67      0.68     17969\n",
      "weighted avg       0.81      0.77      0.79     17969\n",
      " samples avg       0.83      0.80      0.79     17969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('              precision    recall  f1-score   support\\n\\n           0       0.60      0.75      0.67       774\\n           1       0.78      0.70      0.74       653\\n           2       0.85      0.85      0.85       974\\n           3       0.92      0.71      0.80      2769\\n           4       0.65      0.75      0.69       394\\n           5       0.82      0.79      0.80      1086\\n           6       0.85      0.88      0.86      1960\\n           7       0.70      0.59      0.65       622\\n           8       0.91      0.81      0.86       600\\n           9       0.34      0.60      0.43       164\\n          10       0.53      0.46      0.49       335\\n          11       0.59      0.58      0.59       531\\n          12       0.67      0.62      0.65       513\\n          13       0.53      0.30      0.39       102\\n          14       0.85      0.67      0.75       185\\n          15       0.80      0.81      0.80      1347\\n          16       0.36      0.21      0.27        57\\n          17       0.96      0.95      0.95      1641\\n          18       0.93      0.91      0.92      2208\\n          19       0.64      0.59      0.62       444\\n          20       0.65      0.59      0.62       610\\n\\n   micro avg       0.81      0.77      0.79     17969\\n   macro avg       0.71      0.67      0.68     17969\\nweighted avg       0.81      0.77      0.79     17969\\n samples avg       0.83      0.80      0.79     17969\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab60847f016ca05497719c81611e5a3da76ef1597f0d5aec9b876a6fbb687fc2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
